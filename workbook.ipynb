{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization/Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jack/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jack/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/jack/Library/Python/3.8/lib/python/site-packages/pandas/core/internals/blocks.py:993: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  arr_value = np.array(value)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv \n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader, SequentialSampler\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def normalise_text(text): \n",
    "    html_space = re.compile(\"%20\")\n",
    "    newline_pattern =  re.compile(\"\\\\n([^0-9])\")\n",
    "    numeric_pattern = re.compile(\"([0-9]+),([0-9]{3},?)+\")\n",
    "    punctuation_pattern = re.compile(\"[^\\w\\s]\")\n",
    "    url_pattern = re.compile(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\")\n",
    "    normalized_text = text\n",
    "     \n",
    "    while bool(html_space.search(normalized_text)):\n",
    "        normalized_text = re.sub(html_space, r' ', normalized_text)\n",
    "        \n",
    "    while bool(newline_pattern.search(normalized_text)):\n",
    "        normalized_text = re.sub(newline_pattern, r' \\1', normalized_text)\n",
    "\n",
    "    while bool(numeric_pattern.search(normalized_text)):\n",
    "        normalized_text = re.sub(numeric_pattern, r'\\1\\2', normalized_text)\n",
    "        \n",
    "    normalized_text = re.sub(url_pattern, '', normalized_text)\n",
    "    normalized_text = str.lower(normalized_text)\n",
    "    \n",
    "    lines = normalized_text.split('\\n')\n",
    "\n",
    "    lines = [x for x in csv.reader(lines, quotechar='\"', delimiter=',',\n",
    "               quoting=csv.QUOTE_ALL, skipinitialspace=True) if len(x) > 0]\n",
    "    \n",
    "    normalized_lines = []\n",
    "    for line in lines:\n",
    "        normalized_lines.append([re.sub(punctuation_pattern, '', x) for x in line])\n",
    "\n",
    "    return normalized_lines\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Avoid stopword removal - prepositional words are useful for BERT\n",
    "    blacklist = [] # stopwords.words('english')\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = []\n",
    "    if not text:\n",
    "        return tokens\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if any(tokens):\n",
    "        tokens = [x for x in tokens if x not in blacklist]\n",
    "        tokens = [lemmatizer.lemmatize(x) for x in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "max_token_len = 0 \n",
    "    \n",
    "def clean_data(dataframe):\n",
    "    for row in dataframe.iterrows():\n",
    "        row_data = row[1]\n",
    "        keywords = clean_text(row_data.keyword)\n",
    "        text = clean_text(row_data.text) \n",
    "            \n",
    "        sent = ' '.join(text)\n",
    "        sent = ' [CLS] ' + sent + ' [SEP] '\n",
    "        if 'target' in dataframe:\n",
    "            new_row = [row_data.id, keywords, row_data.location, sent, row_data.target]\n",
    "        else:\n",
    "            new_row = [row_data.id, keywords, row_data.location, sent]\n",
    "        dataframe.iloc[row[0]] = new_row\n",
    "    return dataframe\n",
    "\n",
    "def load_text(file):\n",
    "    with open(file) as f:\n",
    "        lines = f.read()\n",
    "    normalised_text = normalise_text(lines)\n",
    "\n",
    "    data = pd.DataFrame(normalised_text[1:], columns=normalised_text[0])\n",
    "    data = clean_data(data)\n",
    "    return data\n",
    "\n",
    "train_data = load_text('./data/train.csv')\n",
    "test_data = load_text('./data/test.csv')\n",
    "train_data.to_csv('./data/normalized_train_data.csv', index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balance of data  0.43 : 0.57 \n"
     ]
    }
   ],
   "source": [
    "no_train_disasters = len(train_data.loc[train_data['target'] == \"1\"])\n",
    "no_train_nondisasters = len(train_data.loc[train_data['target'] == \"0\"])\n",
    "pct_train_disasters = round(no_train_disasters/(no_train_disasters+no_train_nondisasters), 2)\n",
    "pct_no_train_disasters = round(1-pct_train_disasters, 2)\n",
    "print(\"Balance of data  %s : %s \" % (pct_train_disasters, pct_no_train_disasters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Demonstrates a slight data augmentation inbalance, which we will attempt to correct by downsampling.\n",
    "- Data seems to be ordered by category, so we'll shuffle it for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balance of data  0.5 : 0.5 \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "no_indices_to_remove = no_train_nondisasters-no_train_disasters\n",
    "indices = [x for x in train_data.loc[train_data['target'] == \"0\"].index]\n",
    "indices_to_remove = random.sample(indices, no_indices_to_remove)\n",
    "train_data = train_data.drop(indices_to_remove)\n",
    "\n",
    "no_train_disasters = len(train_data.loc[train_data['target'] == \"1\"])\n",
    "no_train_nondisasters = len(train_data.loc[train_data['target'] == \"0\"])\n",
    "pct_train_disasters = round(no_train_disasters/(no_train_disasters+no_train_nondisasters), 2)\n",
    "pct_no_train_disasters = round(1-pct_train_disasters, 2)\n",
    "print(\"Balance of data  %s : %s \" % (pct_train_disasters, pct_no_train_disasters))\n",
    "\n",
    "train_data = train_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6381</th>\n",
       "      <td>9119</td>\n",
       "      <td>[suicide, bomb]</td>\n",
       "      <td>homs syria</td>\n",
       "      <td>[11, soldier, killed, isi, suicide, bomb, air,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3143</th>\n",
       "      <td>4517</td>\n",
       "      <td>[emergency]</td>\n",
       "      <td>kuwait</td>\n",
       "      <td>[plane, new, york, kuwait, diverts, uk, declar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6966</th>\n",
       "      <td>9991</td>\n",
       "      <td>[tsunami]</td>\n",
       "      <td>in the word of god</td>\n",
       "      <td>[author_mike, amen, today, day, salvation, thx...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>229</td>\n",
       "      <td>[airplane, accident]</td>\n",
       "      <td></td>\n",
       "      <td>[expert, france, begin, examining, airplane, d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>2973</td>\n",
       "      <td>[dead]</td>\n",
       "      <td></td>\n",
       "      <td>[beforeitsnews, hundred, feared, dead, libyan,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6477</th>\n",
       "      <td>9264</td>\n",
       "      <td>[sunk]</td>\n",
       "      <td>london england</td>\n",
       "      <td>[still, hasnt, sunk, ive, actually, met, idol]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6406</th>\n",
       "      <td>9157</td>\n",
       "      <td>[suicide, bomber]</td>\n",
       "      <td></td>\n",
       "      <td>[news, islamic, state, claim, suicide, bombing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6253</th>\n",
       "      <td>8935</td>\n",
       "      <td>[snowstorm]</td>\n",
       "      <td>in the spirit world</td>\n",
       "      <td>[photo, mothernaturenetwork, thundersnow, hear...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5073</th>\n",
       "      <td>7231</td>\n",
       "      <td>[natural, disaster]</td>\n",
       "      <td></td>\n",
       "      <td>[top, insurer, blast, lack, australian, govt, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>2446</td>\n",
       "      <td>[collide]</td>\n",
       "      <td></td>\n",
       "      <td>[devia, ler, collide, wattys, 2015, wattpad, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id               keyword             location  \\\n",
       "6381  9119       [suicide, bomb]           homs syria   \n",
       "3143  4517           [emergency]               kuwait   \n",
       "6966  9991             [tsunami]   in the word of god   \n",
       "159    229  [airplane, accident]                        \n",
       "2071  2973                [dead]                        \n",
       "6477  9264                [sunk]       london england   \n",
       "6406  9157     [suicide, bomber]                        \n",
       "6253  8935           [snowstorm]  in the spirit world   \n",
       "5073  7231   [natural, disaster]                        \n",
       "1695  2446             [collide]                        \n",
       "\n",
       "                                                   text target  \n",
       "6381  [11, soldier, killed, isi, suicide, bomb, air,...      1  \n",
       "3143  [plane, new, york, kuwait, diverts, uk, declar...      1  \n",
       "6966  [author_mike, amen, today, day, salvation, thx...      1  \n",
       "159   [expert, france, begin, examining, airplane, d...      1  \n",
       "2071  [beforeitsnews, hundred, feared, dead, libyan,...      1  \n",
       "6477     [still, hasnt, sunk, ive, actually, met, idol]      0  \n",
       "6406  [news, islamic, state, claim, suicide, bombing...      1  \n",
       "6253  [photo, mothernaturenetwork, thundersnow, hear...      1  \n",
       "5073  [top, insurer, blast, lack, australian, govt, ...      1  \n",
       "1695  [devia, ler, collide, wattys, 2015, wattpad, t...      0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape\n",
    "train_data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Model Training\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "labels = [int(x) for x in train_data['target'].tolist()]\n",
    "sents = [tokenizer.tokenize(sent) for sent in train_data['text'].tolist()]\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in sents]\n",
    "input_ids = pad_sequences(input_ids, maxlen=64, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "attention_weights = []\n",
    "for seq in input_ids:\n",
    "  weights = [float(i>0) for i in seq]\n",
    "  attention_weights.append(weights)\n",
    "  \n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_weights, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "batch_size = 32\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407873900/407873900 [14:07<00:00, 481348.51B/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "print('Model loaded!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]/Users/jack/Library/Python/3.8/lib/python/site-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "t = []  \n",
    "train_loss_set = []\n",
    "epochs = 2\n",
    "\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "  # Set model to train mode\n",
    "  model.train()\n",
    "  \n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "      \n",
    "    # Clear dataloader gradients from previous batch\n",
    "    optimizer.zero_grad()\n",
    "      \n",
    "    # Forward pass\n",
    "    loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    train_loss_set.append(loss.item())\n",
    "      \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient step.\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "    \n",
    "print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "# Validation\n",
    "# Put model in evaluation\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "# Evaluate data for one epoch\n",
    "for batch in validation_dataloader:\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    # Informing torch not to store gradients\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        \n",
    "    tmp_eval_accuracy = flat_accuracy(logits, b_labels)\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "    \n",
    "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (disaster_tweets)",
   "language": "python",
   "name": "pycharm-d59fb392"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
