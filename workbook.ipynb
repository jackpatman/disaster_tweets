{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization/Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jack/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jack/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv \n",
    "import string\n",
    "import nltk\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader, SequentialSampler\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    \n",
    "def normalise_text(text): \n",
    "    html_space = re.compile(\"%20\")\n",
    "    newline_pattern =  re.compile(\"\\\\n([^0-9])\")\n",
    "    numeric_pattern = re.compile(\"([0-9]+),([0-9]{3},?)+\")\n",
    "    numbers_pattern = re.compile(\"([0-9])\")\n",
    "    punctuation_pattern = re.compile(\"[^\\w\\s]\")\n",
    "    url_pattern = re.compile(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\")\n",
    "    normalized_text = text\n",
    "     \n",
    "    while bool(html_space.search(normalized_text)):\n",
    "        normalized_text = re.sub(html_space, r' ', normalized_text)\n",
    "        \n",
    "    while bool(newline_pattern.search(normalized_text)):\n",
    "        normalized_text = re.sub(newline_pattern, r' \\1', normalized_text)\n",
    "\n",
    "    while bool(numeric_pattern.search(normalized_text)):\n",
    "        normalized_text = re.sub(numeric_pattern, r'\\1\\2', normalized_text)\n",
    "        \n",
    "    normalized_text = re.sub(url_pattern, '', normalized_text)\n",
    "    normalized_text = str.lower(normalized_text)\n",
    "    \n",
    "    lines = normalized_text.split('\\n')\n",
    "\n",
    "    lines = [x for x in csv.reader(lines, quotechar='\"', delimiter=',',\n",
    "               quoting=csv.QUOTE_ALL, skipinitialspace=True) if len(x) > 0]\n",
    "    \n",
    "    normalized_lines = []\n",
    "    for line in lines:\n",
    "        normalized_lines.append([re.sub(punctuation_pattern, '', x) for x in line])\n",
    "\n",
    "    return normalized_lines\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Avoid stopword removal - prepositional words are useful for BERT\n",
    "    blacklist = [] # stopwords.words('english')\n",
    "    \n",
    "    tokens = []\n",
    "    if not text:\n",
    "        return tokens\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if any(tokens):\n",
    "        tokens = [x for x in tokens if x not in blacklist]\n",
    "        tokens = [lemmatizer.lemmatize(x) for x in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "max_token_len = 0 \n",
    "    \n",
    "def clean_data(dataframe):\n",
    "    for row in dataframe.iterrows():\n",
    "        row_data = row[1]\n",
    "        keywords = clean_text(row_data.keyword)\n",
    "        text = clean_text(row_data.text) \n",
    "        location = row_data.location\n",
    "        keywords = '[SEP] '.join(keywords)\n",
    "        \n",
    "        if not location:\n",
    "            location = ''\n",
    "        \n",
    "        if not keywords:\n",
    "            keywords = ''\n",
    "            \n",
    "        sent = ' '.join(text)\n",
    "        sent = '[CLS] ' + location  + keywords + ' [SEP] ' + sent + ' [SEP]'\n",
    "        #sent = '[CLS] ' + sent + ' [SEP]'\n",
    "        if 'target' in dataframe:\n",
    "            new_row = [row_data.id, keywords, row_data.location, sent, row_data.target]\n",
    "        else:\n",
    "            new_row = [row_data.id, keywords, row_data.location, sent]\n",
    "        dataframe.iloc[row[0]] = new_row\n",
    "    return dataframe\n",
    "\n",
    "def load_text(file):\n",
    "    with open(file) as f:\n",
    "        lines = f.read()\n",
    "    normalised_text = normalise_text(lines)\n",
    "\n",
    "    data = pd.DataFrame(normalised_text[1:], columns=normalised_text[0])\n",
    "    data = clean_data(data)\n",
    "    return data\n",
    "\n",
    "train_data = load_text('./data/train.csv')[1:200]\n",
    "test_data = load_text('./data/test.csv')[1:2]\n",
    "train_data.to_csv('./data/normalized_train_data.csv', index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balance of data  0.48 : 0.52 \n"
     ]
    }
   ],
   "source": [
    "no_train_disasters = len(train_data.loc[train_data['target'] == \"1\"])\n",
    "no_train_nondisasters = len(train_data.loc[train_data['target'] == \"0\"])\n",
    "pct_train_disasters = round(no_train_disasters/(no_train_disasters+no_train_nondisasters), 2)\n",
    "pct_no_train_disasters = round(1-pct_train_disasters, 2)\n",
    "print(\"Balance of data  %s : %s \" % (pct_train_disasters, pct_no_train_disasters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Demonstrates a slight data augmentation inbalance, which we will attempt to correct by downsampling.\n",
    "- Data seems to be ordered by category, so we'll shuffle it for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balance of data  0.5 : 0.5 \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "no_indices_to_remove = max(0, no_train_nondisasters-no_train_disasters)\n",
    "indices = [x for x in train_data.loc[train_data['target'] == \"0\"].index]\n",
    "indices_to_remove = random.sample(indices, no_indices_to_remove)\n",
    "train_data = train_data.drop(indices_to_remove)\n",
    "\n",
    "no_train_disasters = len(train_data.loc[train_data['target'] == \"1\"])\n",
    "no_train_nondisasters = len(train_data.loc[train_data['target'] == \"0\"])\n",
    "pct_train_disasters = round(no_train_disasters/(no_train_disasters+no_train_nondisasters), 2)\n",
    "pct_no_train_disasters = round(1-pct_train_disasters, 2)\n",
    "print(\"Balance of data  %s : %s \" % (pct_train_disasters, pct_no_train_disasters))\n",
    "\n",
    "train_data = train_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape\n",
    "train_data.sample(10)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Model Training\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "labels = [int(x) for x in train_data['target'].tolist()]\n",
    "sents = [tokenizer.tokenize(sent) for sent in train_data['text'].tolist()]\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in sents]\n",
    "input_ids = pad_sequences(input_ids, maxlen=64, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "attention_weights = []\n",
    "for seq in input_ids:\n",
    "  weights = [float(i>0) for i in seq]\n",
    "  attention_weights.append(weights)\n",
    "  \n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.25)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_weights, input_ids,\n",
    "                                             random_state=2018, test_size=0.25)\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "batch_size = 64\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=len(validation_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(train_inputs))\n",
    "print(len(validation_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]/Users/jack/Library/Python/3.8/lib/python/site-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Epoch:  25%|██▌       | 1/4 [01:09<03:27, 69.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Validation F1: 0.8648648648648648\n",
      "Train loss: 0.6515123248100281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 2/4 [02:17<02:16, 68.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Validation F1: 0.8648648648648648\n",
      "Train loss: 0.42458681762218475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 3/4 [03:26<01:09, 69.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Validation F1: 0.8648648648648648\n",
      "Train loss: 0.24942193925380707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [04:38<00:00, 69.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Validation F1: 0.8648648648648648\n",
      "Train loss: 0.1824434151252111\n",
      "Training model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:  25%|██▌       | 1/4 [01:11<03:33, 71.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Validation F1: 0.7547169811320755\n",
      "Train loss: 0.6911155978838602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 2/4 [02:21<02:21, 70.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Validation F1: 0.8444444444444444\n",
      "Train loss: 0.51168093085289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 3/4 [03:32<01:10, 70.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Validation F1: 0.8695652173913044\n",
      "Train loss: 0.30262574553489685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [04:40<00:00, 70.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Validation F1: 0.8717948717948718\n",
      "Train loss: 0.12437142431735992\n",
      "Training model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:  25%|██▌       | 1/4 [01:09<03:27, 69.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Validation F1: 0.6349206349206349\n",
      "Train loss: 0.7475946346918741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 2/4 [02:20<02:20, 70.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Validation F1: 0.7755102040816326\n",
      "Train loss: 0.5771980484326681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 3/4 [03:30<01:10, 70.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Validation F1: 0.7755102040816326\n",
      "Train loss: 0.3776911497116089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 4/4 [04:38<00:00, 69.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Validation F1: 0.9268292682926829\n",
      "Train loss: 0.2767874002456665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = torch.argmax(preds, dim=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    preds_converted = list(map(bool,pred_flat))\n",
    "    labels_converted = list(map(bool,labels_flat))\n",
    "    \n",
    "    f1 = f1_score(y_true=labels_converted, y_pred=preds_converted)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    model=BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=2e-5,\n",
    "                         warmup=.1)\n",
    "    t = []  \n",
    "    train_loss_set = []\n",
    "    epochs = 4\n",
    "\n",
    "    best_val_accuracy = 0\n",
    "    best_train_loss = 1\n",
    "    best_model = None\n",
    "    for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "        # Set model to train mode \n",
    "        model.train()\n",
    "        \n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            # Clear dataloader gradients from previous batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            train_loss_set.append(loss.item())\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient step.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "            train_loss = tr_loss/nb_tr_steps\n",
    "            if train_loss < best_train_loss:\n",
    "                best_train_loss = train_loss\n",
    "\n",
    "        # Validation set \n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            # Informing torch not to store gradients\n",
    "            with torch.no_grad():\n",
    "                logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, b_labels)\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "            val_accuracy = eval_accuracy/nb_eval_steps\n",
    "\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_model = model\n",
    "\n",
    "        print(\"Epoch {}\".format(epoch))\n",
    "        print(\"Validation F1: {}\".format(best_val_accuracy))\n",
    "        print(\"Train loss: {}\".format(best_train_loss))\n",
    "    return best_model\n",
    "\n",
    "\n",
    "print(\"Training model 1\")\n",
    "model_a = train_model()\n",
    "print(\"Training model 2\")\n",
    "model_b = train_model()\n",
    "print(\"Training model 3\")\n",
    "model_c = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "sents = [tokenizer.tokenize(sent) for sent in test_data['text'].tolist()]\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in sents]\n",
    "input_ids = pad_sequences(input_ids, maxlen=64, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "attention_weights = []\n",
    "for seq in input_ids:\n",
    "  weights = [float(i>0) for i in seq]\n",
    "  attention_weights.append(weights)\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "test_masks = torch.tensor(attention_weights)\n",
    "test_dataset = TensorDataset(input_ids, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=len(test_dataset))\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    b_input_ids, b_input_mask = batch\n",
    "\n",
    "    # Informing torch not to store gradients\n",
    "    with torch.no_grad():\n",
    "        logits1 = model_a(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits2 = model_b(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits3 = model_c(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "results1 = torch.argmax(logits1, dim=1)\n",
    "results2 = torch.argmax(logits2, dim=1)\n",
    "results3 = torch.argmax(logits3, dim=1)\n",
    "tf_a = tf.stack([results1, results2, results3])  \n",
    "test_preds = mode(tf_a)[0][0]\n",
    "input_ids = b_input_ids.flatten()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "import csv \n",
    "\n",
    "with open('submission.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    f.write('id,target\\n')\n",
    "    writer.writerows(zip(test_data['id'].tolist(), test_preds.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (disaster_tweets)",
   "language": "python",
   "name": "pycharm-d59fb392"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
