{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load values, perform word-analytics. Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jack/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jack/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/jack/Library/Python/3.8/lib/python/site-packages/pandas/core/internals/blocks.py:993: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.array(value)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv \n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def normalise_text(text): \n",
    "    html_space = re.compile(\"%20\")\n",
    "    newline_pattern =  re.compile(\"\\\\n([^0-9])\")\n",
    "    numeric_pattern = re.compile(\"([0-9]+),([0-9]{3},?)+\")\n",
    "    punctuation_pattern = re.compile(\"[^\\w\\s]\")\n",
    "    url_pattern = re.compile(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\")\n",
    "    normalized_text = text\n",
    "     \n",
    "    while bool(html_space.search(normalized_text)):\n",
    "        normalized_text = re.sub(html_space, r' ', normalized_text)\n",
    "        \n",
    "    while bool(newline_pattern.search(normalized_text)):\n",
    "        normalized_text = re.sub(newline_pattern, r' \\1', normalized_text)\n",
    "\n",
    "    while bool(numeric_pattern.search(normalized_text)):\n",
    "        normalized_text = re.sub(numeric_pattern, r'\\1\\2', normalized_text)\n",
    "        \n",
    "    normalized_text = re.sub(url_pattern, '', normalized_text)\n",
    "    normalized_text = str.lower(normalized_text)\n",
    "    \n",
    "    lines = normalized_text.split('\\n')\n",
    "\n",
    "    lines = [x for x in csv.reader(lines, quotechar='\"', delimiter=',',\n",
    "               quoting=csv.QUOTE_ALL, skipinitialspace=True) if len(x) > 0]\n",
    "    \n",
    "    normalized_lines = []\n",
    "    for line in lines:\n",
    "        normalized_lines.append([re.sub(punctuation_pattern, '', x) for x in line])\n",
    "\n",
    "    return normalized_lines\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    blacklist = stopwords.words('english')\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = []\n",
    "    if not text:\n",
    "        return tokens\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if any(tokens):\n",
    "        tokens = [x for x in tokens if x not in blacklist]\n",
    "        tokens = [lemmatizer.lemmatize(x) for x in tokens]\n",
    "    return tokens\n",
    "    \n",
    "def clean_data(dataframe):\n",
    "    for row in dataframe.iterrows():\n",
    "        row_data = row[1]\n",
    "        keywords = clean_text(row_data.keyword)\n",
    "        text = clean_text(row_data.text)\n",
    "        \n",
    "        if 'target' in dataframe:\n",
    "            new_row = [row_data.id, keywords, row_data.location, text, row_data.target]\n",
    "        else:\n",
    "            new_row = [row_data.id, keywords, row_data.location, text]\n",
    "        dataframe.iloc[row[0]] = new_row\n",
    "    return dataframe\n",
    "\n",
    "def load_text(file):\n",
    "    with open(file) as f:\n",
    "        lines = f.read()\n",
    "    normalised_text = normalise_text(lines)\n",
    "\n",
    "    data = pd.DataFrame(normalised_text[1:], columns=normalised_text[0])\n",
    "    data = clean_data(data)\n",
    "    return data\n",
    "\n",
    "train_data = load_text('./data/train.csv')\n",
    "test_data = load_text('./data/test.csv')\n",
    "train_data.to_csv('./data/normalized_train_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Balance of data  0.43 : 0.57 \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "no_train_disasters = len(train_data.loc[train_data['target'] == \"1\"])\n",
    "no_train_nondisasters = len(train_data.loc[train_data['target'] == \"0\"])\n",
    "pct_train_disasters = round(no_train_disasters/(no_train_disasters+no_train_nondisasters), 2)\n",
    "pct_no_train_disasters = round(1-pct_train_disasters, 2)\n",
    "print(\"Balance of data  %s : %s \" % (pct_train_disasters, pct_no_train_disasters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Demonstrates a slight data augmentation inbalance, which we will attempt to correct by downsampling.\n",
    "- Data seems to be ordered by category, so we'll shuffle it for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Balance of data  0.5 : 0.5 \n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "no_indices_to_remove = no_train_nondisasters-no_train_disasters\n",
    "indices = [x for x in train_data.loc[train_data['target'] == \"0\"].index]\n",
    "indices_to_remove = random.sample(indices, no_indices_to_remove)\n",
    "train_data = train_data.drop(indices_to_remove)\n",
    "\n",
    "no_train_disasters = len(train_data.loc[train_data['target'] == \"1\"])\n",
    "no_train_nondisasters = len(train_data.loc[train_data['target'] == \"0\"])\n",
    "pct_train_disasters = round(no_train_disasters/(no_train_disasters+no_train_nondisasters), 2)\n",
    "pct_no_train_disasters = round(1-pct_train_disasters, 2)\n",
    "print(\"Balance of data  %s : %s \" % (pct_train_disasters, pct_no_train_disasters))\n",
    "\n",
    "train_data = train_data.sample(frac=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (disaster_tweets)",
   "language": "python",
   "name": "pycharm-d59fb392"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}