{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load values, perform word-analytics. Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jack/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jack/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "/Users/jack/Library/Python/3.8/lib/python/site-packages/pandas/core/internals/blocks.py:993: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr_value = np.array(value)\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "         id keyword location  \\\n",
      "0         1      []            \n",
      "1         4      []            \n",
      "2         5      []            \n",
      "3         6      []            \n",
      "4         7      []            \n",
      "...     ...     ...      ...   \n",
      "7608  10869      []            \n",
      "7609  10870      []            \n",
      "7610  10871      []            \n",
      "7611  10872      []            \n",
      "7612  10873      []            \n",
      "\n",
      "                                                   text target  \n",
      "0     [deed, reason, earthquake, may, allah, forgive...      1  \n",
      "1         [forest, fire, near, la, ronge, sask, canada]      1  \n",
      "2     [resident, asked, shelter, place, notified, of...      1  \n",
      "3     [13000, people, receive, wildfire, evacuation,...      1  \n",
      "4     [got, sent, photo, ruby, alaska, smoke, wildfi...      1  \n",
      "...                                                 ...    ...  \n",
      "7608  [two, giant, crane, holding, bridge, collapse,...      1  \n",
      "7609  [aria_ahrary, thetawniest, control, wild, fire...      1  \n",
      "7610            [m194, 0104, utc, 5km, volcano, hawaii]      1  \n",
      "7611  [police, investigating, ebike, collided, car, ...      1  \n",
      "7612  [latest, home, razed, northern, california, wi...      1  \n",
      "\n",
      "[7613 rows x 5 columns]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv \n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def normalise_text(text): \n",
    "    html_space = re.compile(\"%20\")\n",
    "    newline_pattern =  re.compile(\"\\\\n([^0-9])\")\n",
    "    numeric_pattern = re.compile(\"([0-9]+),([0-9]{3},?)+\")\n",
    "    punctuation_pattern = re.compile(\"[^\\w\\s]\")\n",
    "    url_pattern = re.compile(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\")\n",
    "    normalized_text = text\n",
    "     \n",
    "    while bool(html_space.search(normalized_text)):\n",
    "        normalized_text = re.sub(html_space, r' ', normalized_text)\n",
    "        \n",
    "    while bool(newline_pattern.search(normalized_text)):\n",
    "        normalized_text = re.sub(newline_pattern, r' \\1', normalized_text)\n",
    "\n",
    "    while bool(numeric_pattern.search(normalized_text)):\n",
    "        normalized_text = re.sub(numeric_pattern, r'\\1\\2', normalized_text)\n",
    "        \n",
    "    normalized_text = re.sub(url_pattern, '', normalized_text)\n",
    "    normalized_text = str.lower(normalized_text)\n",
    "    \n",
    "    lines = normalized_text.split('\\n')\n",
    "\n",
    "    lines = [x for x in csv.reader(lines, quotechar='\"', delimiter=',',\n",
    "               quoting=csv.QUOTE_ALL, skipinitialspace=True) if len(x) > 0]\n",
    "    \n",
    "    normalized_lines = []\n",
    "    for line in lines:\n",
    "        normalized_lines.append([re.sub(punctuation_pattern, '', x) for x in line])\n",
    "\n",
    "    return normalized_lines\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    blacklist = stopwords.words('english')\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokens = []\n",
    "    if not text:\n",
    "        return tokens\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if any(tokens):\n",
    "        tokens = [x for x in tokens if x not in blacklist]\n",
    "        tokens = [lemmatizer.lemmatize(x) for x in tokens]\n",
    "    return tokens\n",
    "    \n",
    "def clean_data(dataframe):\n",
    "    for row in dataframe.iterrows():\n",
    "        row_data = row[1]\n",
    "        keywords = clean_text(row_data.keyword)\n",
    "        text = clean_text(row_data.text)\n",
    "        \n",
    "        if 'target' in dataframe:\n",
    "            new_row = [row_data.id, keywords, row_data.location, text, row_data.target]\n",
    "        else:\n",
    "            new_row = [row_data.id, keywords, row_data.location, text]\n",
    "        dataframe.iloc[row[0]] = new_row\n",
    "    return dataframe\n",
    "\n",
    "def load_text(file):\n",
    "    with open(file) as f:\n",
    "        lines = f.read()\n",
    "    normalised_text = normalise_text(lines)\n",
    "\n",
    "    data = pd.DataFrame(normalised_text[1:], columns=normalised_text[0])\n",
    "    data = clean_data(data)\n",
    "    return data\n",
    "\n",
    "train_data = load_text('./data/train.csv')\n",
    "#test_data = load_text('./data/test.csv')\n",
    "train_data.to_csv('./data/normalized_train_data.csv', index=False)\n",
    "print(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (disaster_tweets)",
   "language": "python",
   "name": "pycharm-d59fb392"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}