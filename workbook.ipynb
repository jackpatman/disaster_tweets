{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization/Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jack/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jack/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "100%|██████████| 7613/7613 [00:03<00:00, 2268.20it/s]\n",
      "100%|██████████| 3263/3263 [00:01<00:00, 2675.91it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv \n",
    "import string\n",
    "import nltk\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader, SequentialSampler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "def normalise_text(text): \n",
    "    html_space = re.compile(\"%20\")\n",
    "    newline_pattern =  re.compile(\"\\\\n([^0-9])\")\n",
    "    numeric_pattern = re.compile(\"([0-9]+),([0-9]{3},?)+\")\n",
    "    numbers_pattern = re.compile(\"([0-9])\")\n",
    "    random_patterns = re.compile(\"Û_\")\n",
    "    punctuation_marks = re.compile(\"ûª\")\n",
    "    punctuation_pattern = re.compile(\"[^\\w\\s]\")\n",
    "    url_pattern = re.compile(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)\")\n",
    "    normalized_text = text\n",
    "     \n",
    "    while bool(html_space.search(normalized_text)):\n",
    "        normalized_text = re.sub(html_space, r' ', normalized_text)\n",
    "        \n",
    "    while bool(newline_pattern.search(normalized_text)):\n",
    "        normalized_text = re.sub(newline_pattern, r' \\1', normalized_text)\n",
    "\n",
    "    while bool(numeric_pattern.search(normalized_text)):\n",
    "        normalized_text = re.sub(numeric_pattern, r'\\1\\2', normalized_text)\n",
    "    \n",
    "    while bool(punctuation_marks.search(normalized_text)):\n",
    "        normalized_text = re.sub(punctuation_marks, r'', normalized_text)    \n",
    "    \n",
    "    while bool(random_patterns.search(normalized_text)):\n",
    "        normalized_text = re.sub(random_patterns, r'', normalized_text)  \n",
    "        \n",
    "    normalized_text = re.sub(url_pattern, '', normalized_text)\n",
    "    normalized_text = str.lower(normalized_text)\n",
    "    \n",
    "    lines = normalized_text.split('\\n')\n",
    "\n",
    "    lines = [x for x in csv.reader(lines, quotechar='\"', delimiter=',',\n",
    "               quoting=csv.QUOTE_ALL, skipinitialspace=True) if len(x) > 0]\n",
    "    \n",
    "    normalized_lines = []\n",
    "    for line in lines:\n",
    "        normalized_lines.append([re.sub(punctuation_pattern, '', x) for x in line])\n",
    "\n",
    "    return normalized_lines\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # Avoid stopword removal - prepositional words are useful for BERT\n",
    "    blacklist = stopwords.words('english')\n",
    "    \n",
    "    tokens = []\n",
    "    if not text:\n",
    "        return tokens\n",
    "    \n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if any(tokens):\n",
    "        tokens = [x for x in tokens if x not in blacklist]\n",
    "        tokens = [lemmatizer.lemmatize(x) for x in tokens]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "max_token_len = 0 \n",
    "seen_values = []\n",
    "    \n",
    "def clean_data(dataframe):\n",
    "    new_df = []\n",
    "    columns = []\n",
    "    if 'target' in dataframe: \n",
    "        columns=['id','keyword','location','text','target']\n",
    "    else:\n",
    "        columns=['id','keyword','location','text']\n",
    "    \n",
    "    with tqdm(total=len(dataframe), position=0, leave=True) as pbar:\n",
    "        for idx, row_data in dataframe.iterrows():     \n",
    "            pbar.update(1)\n",
    "            keywords = clean_text(row_data.keyword)\n",
    "            text = clean_text(row_data.text) \n",
    "            location = row_data.location\n",
    "            keywords = '[SEP] '.join(keywords)\n",
    "\n",
    "            if not location:\n",
    "                location = ''\n",
    "            else: \n",
    "                location = location + ' [SEP]'\n",
    "\n",
    "            if not keywords:\n",
    "                keywords = ''\n",
    "            else:\n",
    "                keywords = keywords + ' [SEP]'\n",
    "\n",
    "            sent = ' '.join(text)\n",
    "            if sent in seen_values:\n",
    "                continue\n",
    "            else:\n",
    "                seen_values.append(sent)                \n",
    "                sent = '[CLS] ' + location + keywords + sent + ' [SEP]'\n",
    "                if 'target' in dataframe:\n",
    "                    new_row = [row_data.id, keywords, row_data.location, sent, row_data.target]\n",
    "                else:\n",
    "                    new_row = [row_data.id, keywords, row_data.location, sent]\n",
    "                new_df.append(new_row)\n",
    "     \n",
    "        return pd.DataFrame(new_df, columns=columns)\n",
    "\n",
    "def load_text(file):\n",
    "    with open(file) as f:\n",
    "        lines = f.read()\n",
    "    normalised_text = normalise_text(lines)\n",
    "    data = pd.DataFrame(normalised_text[1:], columns=normalised_text[0])\n",
    "    data = clean_data(data) \n",
    "    return data\n",
    "\n",
    "train_data = load_text('./data/train.csv')\n",
    "test_data = load_text('./data/test.csv')[1:3]\n",
    "train_data.to_csv('./data/normalized_train_data.csv', index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balance of data  0.41 : 0.59 \n"
     ]
    }
   ],
   "source": [
    "no_train_disasters = len(train_data.loc[train_data['target'] == \"1\"])\n",
    "no_train_nondisasters = len(train_data.loc[train_data['target'] == \"0\"])\n",
    "pct_train_disasters = round(no_train_disasters/(no_train_disasters+no_train_nondisasters), 2)\n",
    "pct_no_train_disasters = round(1-pct_train_disasters, 2)\n",
    "print(\"Balance of data  %s : %s \" % (pct_train_disasters, pct_no_train_disasters))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Demonstrates a slight data augmentation inbalance, which we will attempt to correct by downsampling.\n",
    "- Data seems to be ordered by category, so we'll shuffle it for good measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balance of data  0.5 : 0.5 \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "no_indices_to_remove = max(0, no_train_nondisasters-no_train_disasters)\n",
    "indices = [x for x in train_data.loc[train_data['target'] == \"0\"].index]\n",
    "indices_to_remove = random.sample(indices, no_indices_to_remove)\n",
    "train_data = train_data.drop(indices_to_remove)\n",
    "\n",
    "no_train_disasters = len(train_data.loc[train_data['target'] == \"1\"])\n",
    "no_train_nondisasters = len(train_data.loc[train_data['target'] == \"0\"])\n",
    "pct_train_disasters = round(no_train_disasters/(no_train_disasters+no_train_nondisasters), 2)\n",
    "pct_no_train_disasters = round(1-pct_train_disasters, 2)\n",
    "print(\"Balance of data  %s : %s \" % (pct_train_disasters, pct_no_train_disasters))\n",
    "\n",
    "train_data = train_data.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5622"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape\n",
    "train_data.sample(10)\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Model Training\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "labels = [int(x) for x in train_data['target'].tolist()]\n",
    "sents = [tokenizer.tokenize(sent) for sent in train_data['text'].tolist()]\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in sents]\n",
    "input_ids = pad_sequences(input_ids, maxlen=64, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "attention_weights = []\n",
    "for seq in input_ids:\n",
    "  weights = [float(i>0) for i in seq]\n",
    "  attention_weights.append(weights)\n",
    "  \n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
    "                                                            random_state=2018, test_size=0.25)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_weights, input_ids,\n",
    "                                             random_state=2018, test_size=0.25)\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "batch_size = 16\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=len(validation_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4216\n",
      "1406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(train_inputs))\n",
    "print(len(validation_inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]/home/jack/.local/lib/python3.8/site-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Epoch:  20%|██        | 1/5 [00:38<02:32, 38.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Validation F1: 0.7926136363636362\n",
      "Train loss: 0.5199915481110414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 2/5 [01:16<01:55, 38.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Validation F1: 0.7969924812030076\n",
      "Train loss: 0.342666357755661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 3/5 [01:56<01:17, 38.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Validation F1: 0.7969924812030076\n",
      "Train loss: 0.22512169182300568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 4/5 [02:34<00:38, 38.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Validation F1: 0.7969924812030076\n",
      "Train loss: 0.09553971886634827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [03:13<00:00, 38.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "Validation F1: 0.7969924812030076\n",
      "Train loss: 0.09553971886634827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:  20%|██        | 1/5 [00:38<02:35, 38.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Validation F1: 0.7886676875957122\n",
      "Train loss: 0.5141381154570616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 2/5 [01:18<01:57, 39.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Validation F1: 0.8005657708628006\n",
      "Train loss: 0.360504408677419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 3/5 [01:56<01:17, 38.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Validation F1: 0.8005657708628006\n",
      "Train loss: 0.35335104167461395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 4/5 [02:35<00:38, 38.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Validation F1: 0.8005657708628006\n",
      "Train loss: 0.21859297156333923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [03:13<00:00, 38.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "Validation F1: 0.8005657708628006\n",
      "Train loss: 0.13553287088871002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Epoch:  20%|██        | 1/5 [00:39<02:37, 39.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Validation F1: 0.7919366450683946\n",
      "Train loss: 0.5139452720794714\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import f1_score\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    \n",
    "    preds_converted = list(map(bool,pred_flat))\n",
    "    labels_converted = list(map(bool,labels_flat))\n",
    "    \n",
    "    f1 = f1_score(y_true=labels_converted, y_pred=preds_converted)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    model=BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    model.cuda()\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=2e-6,\n",
    "                         warmup=.1)\n",
    "    t = []  \n",
    "    train_loss_set = []\n",
    "    epochs = 5\n",
    "\n",
    "    best_val_accuracy = 0\n",
    "    best_train_loss = 1\n",
    "    best_model = None\n",
    "    for epoch in trange(epochs, desc=\"Epoch\"):\n",
    "        torch.cuda.empty_cache()\n",
    "        # Set model to train mode \n",
    "        model.train()\n",
    "        \n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            # Clear dataloader gradients from previous batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            train_loss_set.append(loss.item())\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient step.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "            train_loss = tr_loss/nb_tr_steps\n",
    "            if train_loss < best_train_loss:\n",
    "                best_train_loss = train_loss\n",
    "\n",
    "        # Validation set \n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            # Informing torch not to store gradients\n",
    "            with torch.no_grad():\n",
    "                logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            \n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "            val_accuracy = eval_accuracy/nb_eval_steps\n",
    "\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                best_model = model\n",
    "\n",
    "        print(\"Epoch {}\".format(epoch))\n",
    "        print(\"Validation F1: {}\".format(best_val_accuracy))\n",
    "        print(\"Train loss: {}\".format(best_train_loss))\n",
    "        \n",
    "    model.to('cpu')\n",
    "    best_model.to('cpu')\n",
    "         \n",
    "    return best_model\n",
    "\n",
    "\n",
    "print(\"Training model 1\")\n",
    "model_a = train_model()\n",
    "print(\"Training model 2\")\n",
    "model_b = train_model()\n",
    "print(\"Training model 3\")\n",
    "model_c = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "sents = [tokenizer.tokenize(sent) for sent in test_data['text'].tolist()]\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in sents]\n",
    "input_ids = pad_sequences(input_ids, maxlen=64, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "attention_weights = []\n",
    "for seq in input_ids:\n",
    "  weights = [float(i>0) for i in seq]\n",
    "  attention_weights.append(weights)\n",
    "\n",
    "input_ids = torch.tensor(input_ids)\n",
    "test_masks = torch.tensor(attention_weights)\n",
    "test_dataset = TensorDataset(input_ids, test_masks)\n",
    "test_sampler = SequentialSampler(test_dataset)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=len(test_dataset))\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    b_input_ids, b_input_mask = batch\n",
    "\n",
    "    # Informing torch not to store gradients\n",
    "    with torch.no_grad():\n",
    "        logits1 = model_a(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits2 = model_b(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits3 = model_c(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "results1 = np.argmax(logits1, axis=1)\n",
    "results2 = np.argmax(logits2, axis=1)\n",
    "results3 = np.argmax(logits3, axis=1)\n",
    "tf_a = tf.stack([results1, results2, results3])  \n",
    "test_preds = mode(tf_a)[0][0]\n",
    "input_ids = b_input_ids.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv \n",
    "\n",
    "with open('submission.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    f.write('id,target\\n')\n",
    "    writer.writerows(zip(test_data['id'].tolist(), test_preds.tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
